---
layout: fpost
title: "Project 1"
permalink: /fpost/1/
#date:   2017-05-26 15:05:55 +0300
image:  /assets/images/blog/post-5.jpg
author: Jongwan Kim
tags:   Autonomy, Object detection, Segmentation, Model Compression, Optimization
---

### 1. Object Detection: Network Design and Optimization

#### YOLOv5 Model Performance Improvement and Pruning
- \- Conducted pruning for YOLOv5-based model performance improvement and lightweight design.

<figure>
  <div style="text-align:center">
    <img src="\fpost\images\pf1\yolov5 capture0.png" alt="대체 텍스트">
  </div>
  <figcaption style="text-align:center">이미지 설명</figcaption>
</figure>

<figure>
  <div style="text-align:center">
    <img src="\fpost\images\pf1\yolov5 capture1.png" alt="대체 텍스트">
  </div>
  <figcaption style="text-align:center">이미지 설명</figcaption>
</figure>


#### Class Imbalanced Problem Solution
- \- Solved class imbalance problem by reflecting the numerical measurement of data overlap (effective number) in the loss function.

<figure>
  <div style="text-align:center">
    <img src="\fpost\images\pf1\class imbalanced problem.png" alt="대체 텍스트">
  </div>
  <figcaption style="text-align:center">이미지 설명</figcaption>
</figure>

### 2. Segmentation Model: Network Design and Optimization

#### Model Exploration and Adaptation
- \- Utilized various models such as DDRNet, DeepLab V3+, and ESPNet.
- \- Optimized models for autonomous driving situations through lightweight design and structural optimization.

<style>
table {
  margin-left: auto;
  margin-right: auto;
  border-collapse: collapse;
  width: 70%;
}
th, td {
  border: 1px solid black;
  padding: 8px;
  text-align: center;
}
th {
  background-color: #f2f2f2;
}

td:nth-child(1) {
  width: 25%;
}
td:nth-child(2) {
  width: 20%;
}
td:nth-child(3) {
  width: 20%;
}
td:nth-child(4) {
  width: 40%;
}
</style>


#### Model Comparison

| Model       | Model Size | Inference Speed (FPS) | Accuracy (NYUv2 dataset) |
|-------------|------------|-----------------------|--------------------------|
| LiteSeg     | 1.2 MB     | 88.2                  | 41.7 mIoU                |
| LiteDepth   | 0.7 MB     | 118.9                 | 0.66 RMSE                |
| EfficientPS | 4.3 MB     | 23.5                  | 42.3 mIoU / 0.83 RMSE    |
| FastDepthSeg| 1.9 MB     | 140.8                 | 41.5 mIoU / 0.83 RMSE    |
| DDRNet      | 7.5 MB     | 82.7                  | 46.2 mIoU / 0.79 RMSE    |
| DeepLab V3+ | 8.4 MB     | 37.6                  | 43.9 mIoU                |


### 3. Model Quantization and Comparison
- \- Applied static quantization, dynamic quantization, and quantization-aware training.
- \- Compared the performance of different models.

#### Quantization Results

<style>
table {
  margin-left: auto;
  margin-right: auto;
  border-collapse: collapse;
  width: 80%;
}
th, td {
  border: 1px solid black;
  padding: 8px;
  text-align: center;
}
th {
  background-color: #f2f2f2;
}

td:nth-child(1) {
  width: 15%;
}
td:nth-child(2) {
  width: 40%;
}
td:nth-child(3) {
  width: 40%;
}
td:nth-child(4) {
  width: 40%;
}
</style>


| Model       | Quantization-aware Training     | Post Static Quantization       | Post Dynamic Quantization      |
|-------------|:--------------------------------:|:-------------------------------:|:--------------------------------:|
|             | mIoU / RMSE / Model Size / FPS   | mIoU / RMSE / Model Size / FPS | mIoU / RMSE / Model Size / FPS |
|-------------|---------------------------------|--------------------------------|---------------------------------|
| LiteSeg     | 40.5 / - / 0.6 MB / 172.4       | 39.1 / - / 0.5 MB / 194.5      | 40.1 / - / 0.6 MB / 178.7       |
| LiteDepth   | - / 0.68 / 0.35 MB / 231.7      | - / 0.70 / 0.3 MB / 248.6      | - / 0.69 / 0.35 MB / 235.9      |
| EfficientPS | 41.8 / 0.85 / 2.15 MB / 45.9    | 40.2 / 0.87 / 1.7 MB / 57.1    | 41.2 / 0.86 / 2.15 MB / 51.3    |
| FastDepthSeg| 40.9 / 0.85 / 0.95 MB / 274.6   | 39.5 / 0.87 / 0.8 MB / 294.8   | 40.3 / 0.86 / 0.95 MB / 281.3   |
| DDRNet      | 44.9 / 0.81 / 3.75 MB / 160.2   | 43.6 / 0.83 / 3.2 MB / 185.3   | 44.3 / 0.82 / 3.75 MB / 171.4   |
| DeepLab V3+ | 42.5 / - / 4.2 MB / 72.9        | 41.0 / - / 3.5 MB / 87.4       | 42.1 / - / 4.2 MB / 79.6        |

| Model       | % Improvement: Quant-aware Train| % Improvement: Static Quant    | % Improvement: Dynamic Quant   |
|-------------|---------------------------------|--------------------------------|---------------------------------|
| LiteSeg     | 95.1% / 1.95x                   | 97.5% / 2.20x                  | 95.8% / 2.02x                   |
| LiteDepth   | 50.0% / 1.95x                   | 42.9% / 2.10x                  | 50.0% / 1.99x                   |
| EfficientPS | 50.0% / 1.95x                   | 39.5% / 2.43x                  | 50.0% / 2.17x                   |
| FastDepthSeg| 50.0% / 1.95x                   | 42.1% / 2.10x                  | 50.0% / 1.99x                   |
| DDRNet      | 50.0% / 1.94x                   | 42.7% / 2.24x                  | 50.0% / 2



#### ONNX Conversion and Optimization
- \- Performed ONNX conversion and optimization for better deployment

#### TensorRT Optimization for Nvidia Xavier Environment

##### Nvidia Xavier Specifications
- GPU: Volta architecture with 512 CUDA cores
- CPU: 8-core ARMv8.2 64-bit processor
- Memory: 16 GB 256-bit LPDDR4x at 2133 MHz

- Optimized models using TensorRT for inference on Nvidia Xavier devices
- Achieved significant improvement in inference speed while maintaining acceptable accuracy levels

### Legacy Code Refactoring

#### GStreamer Pipeline Optimization
- Optimized GStreamer-based pipeline

#### Decoding Improvement
- Replaced SW decoding with HW decoding to reduce CPU overhead

#### Sensor Input and Modularization
- Directed various sensor inputs to the HW decoding module
- Modularized GStreamer and improved the deep learning model loading process using plugin loader

### Weakly Supervised Instance Segmentation Model Development

- Developed weakly supervised instance segmentation model using semantic segmentation data