---
layout: post
title: "[Paper review] Training data-efficient image transformers & distillation through attention"
last_modified_at: 2025-01-16
mathjax: true
image:  /assets/images/blog/post-5.jpg
categories:
  - 논문리뷰
tags:
  - ViT
  - Computer Vision
  - Self-Supervised Learning
  - AI
excerpt: "LOST paper review"
use_math: true
classes: wide
---


> [[Paper](https://arxiv.org/abs/2012.12877)] [[Github](https://github.com/valeoai/LOST)]  
> Touvron, Hugo and Cord, Matthieu and Douze, Matthijs and Massa, Francisco and Sablayrolles, Alexandre and Jegou, Herve
> Facebook AI | Sorbonne University  
> ICML 2021


