---
layout: post
title: "[Paper review] Text Is MASS: Modeling as Stochastic Embedding for Text-Video Retrieval"
last_modified_at: 2025-01-16
mathjax: true
image:  /assets/images/blog/post-5.jpg
categories:
  - 논문리뷰
tags:
  - ViT
  - Computer Vision
  - Video
  - AI
excerpt: "Text is Mass paper review"
use_math: true
classes: wide
---
> arXiv 2023. [[Paper](https://arxiv.org/pdf/2403.17998)] [[Github](https://github.com/Jiamian-Wang/T-MASS-text-video-retrieval)]  
> Jiamian Wang, Guohao Sun, Pichao Wang, Dongfang Liu, Sohail Dianat, Majid Rabbani, Raghuveer Rao, Zhiqiang Tao  
> Rochester Institute of Technology | Amazon Prime Video | Army Research Laboratory  
> CVPR 2024


### 0. Abstract
- 기존 text-video retrieval 방식에서 단일 텍스트 embedding이 비디오의 복잡한 의미를 충분히 표현하기 어려운 문제를 해결하기 위해 새로운 확률적 텍스트 모델링 기법(T-MASS) 제안
- T-MASS는 텍스트를 단일 포인트가 아닌 확률적 text mass로 모델링하여 보다 유연하고 풍부한 의미 표현이 가능하도록 함
- Similarity-aware radius module 도입으로 text-video pair에 맞춰 text mass의 크기를 동적으로 조절
- Support text regularization을 통해 학습 시 text mass를 효과적으로 제어
- T-MASS는 기존 방식 대비 R@1에서 3~6% 성능향상 및 MSRVTT, LSMDC, DiDeMo, Charades, VATEX 데이터셋에서 SOTA 달성

---

<figure>
  <div style="text-align:center">
    <img src="/assets/img/t_mass/fig1.png" alt="Fig 1" style="width:70%;">
  </div>
</figure>

### 1. Introduction
영상 데이터의 폭발적인 증가로 인해 text-video retrieval 연구가 활발히 진행되고 있다. 기존 접근 방식은 텍스트와 비디오를 동일한 embedding 공간에 매핑하여 유사도를 계산하는 방식이 주류를 이루고 있다. 하지만 기존 데이터셋의 텍스트는 짧고 간결하여 비디오의 다양한 의미를 온전히 반영하기 어려운 문제가 있다.

이러한 한계를 극복하기 위해 본 연구에서는 텍스트 embedding을 단일 포인트가 아닌 확률적 질량(probabilistic mass)으로 모델링하는 **T-MASS (Text Modeled As a Stochastic embedding)** 방법을 제안한다. 이를 통해:
1. **비디오의 다양한 의미를 보다 유연하게 표현 가능**
2. **텍스트-비디오 embedding 간의 alignment 문제 완화**
3. **불확실성까지 반영하는 효과적인 검색 가능**

기존 방법과의 차별점으로, T-MASS는 단순한 텍스트 포인트가 아닌 분포(distribution) 형태로 텍스트를 모델링하며, 유사도 기반 반경 조절 모듈(similarity-aware radius module)과 support text vector를 활용하여 효과적인 학습 및 추론을 수행한다.

---

### 2. Method
#### 2.1. Preliminaries
- 텍스트를 $t$, 원본 비디오 클립을 $v$라고 정의
- Text-video retrieval은 $t, v \in \mathbb{R}^d$의 공통 embedding 공간을 학습하는 과정
- Cosine similarity 등 similarity 측정 함수 $s(t, v)$를 사용하여 텍스트-비디오 간 연관성을 평가
- 손실 함수는 **Symmetric Cross Entropy**를 사용하여 관련된 쌍은 가깝게, 무관한 쌍은 멀어지도록 학습

$$
\begin{equation}
L_{t \to v} = -\sum_{i=1}^{N} \log \frac{e^{s(t_i, v_i) \cdot \lambda}}{\sum_{j} e^{s(t_i, v_j) \cdot \lambda}} \\
L_{v \to t} = -\sum_{i=1}^{N} \log \frac{e^{s(t_i, v_i) \cdot \lambda}}{\sum_{j} e^{s(t_j, v_i) \cdot \lambda}} \\
L_{ce} = \frac{1}{2} (L_{t \to v} + L_{v \to t})
\end{equation}
$$

- 손실 함수는 모든 관련 텍스트-비디오 쌍의 유사도가 1, 무관한 쌍의 유사도가 0이 되는 이상적인 상태를 목표로 함
<br>

#### 2.2. Text-Video Representations

##### **Feature Extraction**
**CLIP 기반 텍스트-비디오 검색 방법론 활용**
  - 최근 연구들은 CLIP의 강력한 표현력을 활용하여 텍스트와 비디오 간의 검색 성능을 향상시키고 있음
  - 비디오는 $T$ 개의 프레임으로 구성되며, 일부 프레임 $T'$ 만을 샘플링하여 CLIP에 입력.
  - CLIP의 이미지 인코더 $\phi_v$ 와 텍스트 인코더 $\phi_t$ 를 사용하여 특징을 추출
    $$
    \mathbf{f}_i = \phi_v(f_i), \quad i = 1, ..., T'  \; t = \phi_t(t)
    $$

**프레임-텍스트 융합을 통한 최종 비디오 표현 생성**
  - 기존 연구들은 프레임 특징 $\{f_1, ..., f_{T'}\}$ 를 다양한 방식으로 결합하여 최종 비디오 임베딩을 생성:
    $$
    v = \psi([f_1, ..., f_{T'}], t)
    $$
    - $\psi(\cdot)$ : 프레임-텍스트 상호작용을 활용하는 융합 모듈.

### Contrastive Loss for Text-Video Matching
- 일반적으로 대칭적인 contrastive loss를 사용하여 텍스트와 비디오의 정합도를 극대화:
  $$
  L_{ce} = \frac{1}{2} (L_{t \to v} + L_{v \to t})
  $$
  - $L_{t \to v}$ : 텍스트에서 비디오로의 매칭 손실
  - $L_{v \to t}$ : 비디오에서 텍스트로의 매칭 손실
  - 텍스트-비디오 쌍이 완전히 일치하면 $s(t_i, v_i) = 1$, 그렇지 않으면 $s(t_i, v_j) = 0$.

- 하지만 텍스트 $t$ 는 비디오 $v$ 를 완전히 설명하지 못하며, 표현력 부족 문제로 인해 정확한 정렬이 어려움.


<br>

##### Motivation
- 기존 연구들은 비디오 임베딩 학습($v$)에 집중했으며, 텍스트 표현력 부족 문제를 충분히 해결하지 못함
- 텍스트 $t$는 비디오 $v$에 비해 표현력이 낮으며, 비디오가 제공하는 풍부한 단서를 온전히 반영하기 어려움
- 이를 해결하기 위해 **텍스트를 단일 포인트가 아닌 확률적 질량(text mass)으로 모델링하는 방법을 제안**
<br>

#### 2.3. Proposed Method: **T-MASS**
##### 🔹 Stochastic Text Modeling
- 기존의 텍스트 임베딩 방식과 달리, **텍스트를 단일 점이 아니라 확률적 질량**으로 간주
- **텍스트의 확장된 표현력 확보를 위해 확률적 변형 적용**
  $$
  t_s = t + R \cdot \epsilon, \quad \epsilon \sim P
  $$
  - $\epsilon$: 정규 분포 $P = \mathcal{N}(0,1)$ 에서 샘플링된 노이즈 변수.
  - $R$: 텍스트 질량의 반경을 정의하며 학습 가능.

- **기존 방법과 차별점**
  - 일반적인 CLIP 기반 접근법은 **고정된 점 (point embedding)** 을 사용하여 비디오와 매칭.
  - T-MASS는 **확률적 분포**를 통해 **텍스트 표현의 유연성을 높이고, 의미적으로 더 강건한 매칭**을 수행.
<br>

##### 🔹 Similarity-Aware Radius Modeling
- **텍스트 질량의 반경 $R$을 동적으로 조절하는 모듈 도입**
- 비디오-텍스트의 코사인 유사도를 기반으로 반경을 학습:
  $$
  S_i = s(t, f_i), \quad i = 1, ..., T'
  $$
  $$
  R = \exp\left( \frac{\theta}{T'} \sum_{i=1}^{T'} S_i \right)
  $$
  - $\theta$: 학습 가능한 스칼라 파라미터.

- **다양한 반경 조절 기법 실험**
  - 선형 변환을 추가하여 반경을 더욱 유연하게 학습:
    $$
    R = \exp(S W), \quad S = [S_1, ..., S_{T'}]
    $$
    - $W \in \mathbb{R}^{T' \times d}$: 학습 가능한 가중치 행렬.


<figure>
  <div style="text-align:center">
    <img src="/assets/img/t_mass/fig3.png" alt="Fig 3" style="width:70%;">
  </div>
</figure>


##### 🔹 Learning Text Mass in Joint Space
- 기존 contrastive loss $L_{ce}$를 개선하여 **확률적 텍스트 임베딩을 활용하는 새로운 학습 방법 도입**
  - 텍스트 질량 내의 여러 지점을 샘플링하여 학습
  - 기존 CLIP 손실에 확률적 임베딩을 반영한 새로운 손실 함수 정의
    $$
    L_{\text{total}} = L_{ce} + L_s
    $$
    - $L_s$: 확률적 텍스트 임베딩 $t_s$ 를 사용한 contrastive loss.

- **Support Text Regularization 추가**
  - 텍스트 질량의 스케일과 이동을 조절하기 위해 **"Support Text Embedding" $t_{\text{sup}}$** 를 도입
    $$
    t_{\text{sup}} = t + \frac{v - t}{|v - t|} R
    $$
  - 새로운 contrastive loss 추가:
    $$
    L_{\text{sup}}
    $$
  - 최종 손실 함수:
    $$
    L_{\text{total}} = L_s + \alpha L_{\text{sup}}
    $$
    - $\alpha$: regularization 가중치.

<figure>
  <div style="text-align:center">
    <img src="/assets/img/t_mass/fig4.png" alt="Fig 4" style="width:70%;">
  </div>
</figure>

##### # 🔍 Inference Pipeline

**Step 1: Text-Video Feature Extraction**
- 주어진 텍스트-비디오 쌍 $\{t, v\}$ 에 대해, 먼저 CLIP 기반 텍스트 및 프레임 특징을 추출
  $$
  f_i = \phi_v(f_i), \quad i = 1, ..., T'
  $$
  $$
  t = \phi_t(t)
  $$
- 이후, **프레임 특징들을 융합**하여 최종 비디오 임베딩을 생성:
  $$
  v = \psi([f_1, ..., f_{T'}], t)
  $$
  - $\psi(\cdot)$ : 프레임-텍스트 상호작용을 활용하는 융합 모듈.

**Step 2: Stochastic Sampling of Text Embeddings**
- 텍스트 표현을 단일 포인트가 아닌 확률적 질량으로 모델링
- 이를 위해, Eq. (5)에서 정의한 확률적 변형을 $M$ 번 샘플링하여 여러 개의 텍스트 임베딩 생성
  $$
  \{ t_1^s, ..., t_M^s \}, \quad t_i^s = t + R \cdot \epsilon_i, \quad \epsilon_i \sim \mathcal{N}(0,1)
  $$
- $M$ 개의 샘플링된 텍스트 임베딩을 통해 **다양한 표현 가능성을 탐색**.


**Step 3: Optimal Text Embedding Selection**
- 비디오와 가장 유사한 텍스트 표현 선택
  $$
  t^s = \arg\max_{t_i^s} s(t_i^s, v), \quad i = 1, ..., M
  $$
  - $s(t, v)$: 텍스트와 비디오 간 유사도를 나타내는 함수
  - $t^s$: 최적의 확률적 텍스트 표현


---

### 3. Experiment

#### 3.1. 성능 비교
- 기존 방식 대비 **R@1 성능 3%~6.3% 향상**
- **MSRVTT, LSMDC, DiDeMo, Charades, VATEX 등 5개 벤치마크에서 SOTA 달성**
<br>

#### 3.2. Ablation Study
- **유사도 기반 반경 조절 모듈 추가 시 성능 향상 확인**
- **Support text regularization 적용 시 retrieval 성능 개선**
<br>

#### 3.3. 의미 정렬 분석
- 기존 방식 대비, **T-MASS는 텍스트-비디오 embedding 간의 의미적 정렬이 더 잘 이루어짐을 실험적으로 확인**
- 특히 **복잡한 영상일수록 T-MASS가 더 높은 성능을 보임**

---

## Conclusion
- 기존 text-video retrieval의 한계를 극복하기 위해 **확률적 텍스트 모델링 방법인 T-MASS를 제안**
- **텍스트를 단일 포인트가 아닌 질량으로 모델링하여 보다 풍부한 의미 표현 가능**
- **유사도 기반 반경 조절 모듈과 support text regularization을 활용하여 학습 효과 극대화**
- 기존 대비 높은 성능을 기록하며, 다양한 벤치마크 데이터셋에서 SOTA 달성
- 향후 연구 방향으로는 **비디오 내 프레임 간 의미 변화를 보다 정교하게 반영


### Abstract

