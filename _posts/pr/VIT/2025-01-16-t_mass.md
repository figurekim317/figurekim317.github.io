---
layout: post
title: "[Paper review] Text Is MASS: Modeling as Stochastic Embedding for Text-Video Retrieval"
last_modified_at: 2025-01-16
mathjax: true
image:  /assets/images/blog/post-5.jpg
categories:
  - 논문리뷰
tags:
  - ViT
  - Computer Vision
  - Video
  - AI
excerpt: "Text is Mass paper review"
use_math: true
classes: wide
---
> arXiv 2023. [[Paper](https://arxiv.org/pdf/2403.17998)] [[Github](https://github.com/Jiamian-Wang/T-MASS-text-video-retrieval)]  
> Jiamian Wang, Guohao Sun, Pichao Wang, Dongfang Liu, Sohail Dianat, Majid Rabbani, Raghuveer Rao, Zhiqiang Tao  
> Rochester Institute of Technology | Amazon Prime Video | Army Research Laboratory  
> CVPR 2024


### 0. Abstract
- 기존 text-video retrieval 방식에서 단일 텍스트 embedding이 비디오의 복잡한 의미를 충분히 표현하기 어려운 문제를 해결하기 위해 새로운 확률적 텍스트 모델링 기법(T-MASS) 제안
- T-MASS는 텍스트를 단일 포인트가 아닌 확률적 text mass로 모델링하여 보다 유연하고 풍부한 의미 표현이 가능하도록 함
- Similarity-aware radius module 도입으로 text-video pair에 맞춰 text mass의 크기를 동적으로 조절
- Support text regularization을 통해 학습 시 text mass를 효과적으로 제어
- T-MASS는 기존 방식 대비 R@1에서 3~6% 성능향상 및 MSRVTT, LSMDC, DiDeMo, Charades, VATEX 데이터셋에서 SOTA 달성

---

<figure>
  <div style="text-align:center">
    <img src="/assets/img/t_mass/fig1.png" alt="Fig 1" style="width:70%;">
  </div>
</figure>

### 1. Introduction
영상 데이터의 폭발적인 증가로 인해 text-video retrieval 연구가 활발히 진행되고 있다. 기존 접근 방식은 텍스트와 비디오를 동일한 embedding 공간에 매핑하여 유사도를 계산하는 방식이 주류를 이루고 있다. 하지만 기존 데이터셋의 텍스트는 짧고 간결하여 비디오의 다양한 의미를 온전히 반영하기 어려운 문제가 있다.

이러한 한계를 극복하기 위해 본 연구에서는 텍스트 embedding을 단일 포인트가 아닌 확률적 질량(probabilistic mass)으로 모델링하는 **T-MASS (Text Modeled As a Stochastic embedding)** 방법을 제안한다. 이를 통해:
1. **비디오의 다양한 의미를 보다 유연하게 표현 가능**
2. **텍스트-비디오 embedding 간의 alignment 문제 완화**
3. **불확실성까지 반영하는 효과적인 검색 가능**

기존 방법과의 차별점으로, T-MASS는 단순한 텍스트 포인트가 아닌 분포(distribution) 형태로 텍스트를 모델링하며, 유사도 기반 반경 조절 모듈(similarity-aware radius module)과 support text vector를 활용하여 효과적인 학습 및 추론을 수행한다.

---

### 2. Method
#### 2.1. Preliminaries
- 텍스트를 $t$, 원본 비디오 클립을 $v$라고 정의
- Text-video retrieval은 $t, v \in \mathbb{R}^d$의 공통 embedding 공간을 학습하는 과정
- Cosine similarity 등 similarity 측정 함수 $s(t, v)$를 사용하여 텍스트-비디오 간 연관성을 평가
- 손실 함수는 **Symmetric Cross Entropy**를 사용하여 관련된 쌍은 가깝게, 무관한 쌍은 멀어지도록 학습

$$
\begin{equation}
L_{t \to v} = -\sum_{i=1}^{N} \log \frac{e^{s(t_i, v_i) \cdot \lambda}}{\sum_{j} e^{s(t_i, v_j) \cdot \lambda}} \\
L_{v \to t} = -\sum_{i=1}^{N} \log \frac{e^{s(t_i, v_i) \cdot \lambda}}{\sum_{j} e^{s(t_j, v_i) \cdot \lambda}} \\
L_{ce} = \frac{1}{2} (L_{t \to v} + L_{v \to t})
\end{equation}
$$

- 손실 함수는 모든 관련 텍스트-비디오 쌍의 유사도가 1, 무관한 쌍의 유사도가 0이 되는 이상적인 상태를 목표로 함
<br>

#### 2.2. Text-Video Representations

##### **Feature Extraction**
  - 최근 연구들은 CLIP의 강력한 표현력을 활용하여 텍스트와 비디오 간의 검색 성능을 향상시키고 있음
  - 비디오는 $T$ 개의 프레임으로 구성되며, 일부 프레임 $T'$ 만을 샘플링하여 CLIP에 입력.
  - CLIP의 이미지 인코더 $\phi_v$ 와 텍스트 인코더 $\phi_t$ 를 사용하여 특징을 추출
    $$
    \mathbf{f}_i = \phi_v(f_i), \quad i = 1, ..., T'  \quad \mathbf{t} = \phi_t(t)
    $$
  - 기존 연구들은 프레임 특징 ${ \mathbf{f}_1, ..., \mathbf{f}_{T'}}$ 를 다양한 방식으로 결합하여 최종 비디오 임베딩을 생성:
    $$
    \mathbf{v} = \psi([\mathbf{f}_1, ..., \mathbf{f}_{T'}], \text{;} \mathbf{t})
    $$
    - $\psi(\cdot)$ : 프레임-텍스트 상호작용을 활용하는 융합 모듈.
<br>

##### **Motivation**
- 기존 연구들은 비디오 임베딩 학습($\mathbf{v}$ including frame sampling protocol, $\phi_v(\cdot)$, $\psi(\cdot)$) 에 집중했으며, 텍스트 표현력 부족 문제를 충분히 해결하지 못함
- 텍스트 $\mathbf{t}$는 비디오 $\mathbf{v}$에 비해 표현력이 낮으며, 비디오가 제공하는 풍부한 단서를 온전히 반영하기 어려움
- 이를 해결하기 위해 **텍스트를 단일 포인트가 아닌 확률적 질량(text mass)으로 모델링하는 방법을 제안**
<br>

#### 2.3. Proposed Method: **T-MASS**
##### **Stochastic Text Modeling**
- 기존의 텍스트 임베딩 방식과 달리, **텍스트를 단일 점이 아니라 확률적 질량**으로 간주
- 텍스트의 확장된 표현력 확보를 위해 확률적 변형 적용
  $$
  \mathbf{t}_s = \mathbf{t} + R \cdot \epsilon, \quad \epsilon \sim P
  $$
  - $\epsilon$: 정규 분포 $P = \mathcal{N}(0,1)$ 에서 샘플링된 노이즈 변수.
  - $R$: 텍스트 질량의 반경을 정의하며 학습 가능.

- **기존 방법과 차별점**
  - 일반적인 CLIP 기반 접근법은 고정된 점 (point embedding) 을 사용하여 비디오와 매칭
  - T-MASS는 **확률적 분포**를 통해 **텍스트 표현의 유연성을 높이고, 의미적으로 더 강건한 매칭**을 수행
<br>

##### **Similarity-Aware Radius Modeling**
- 텍스트 질량의 반경 $R$을 동적으로 조절하는 모듈 도입
- 비디오-텍스트의 cosine similarity를 기반으로 반경을 학습:
  $$
  S_i = s(\mathbf{t}, \mathbf{f}_i), \quad i = 1, ..., T', \quad 
  $$
  $$
  R = \exp\left( \frac{\theta}{T'} \sum_{i=1}^{T'} S_i \right)
  $$
  - $\theta$: 학습 가능한 스칼라 파라미터.

- 다양한 반경 조절 기법 실험
  - 선형 변환을 추가하여 반경을 더욱 유연하게 학습:
    $$
    R = \exp(\mathbf{S} \mathbf{W}), \quad \mathbf{S} = [S_1, ..., S_{T'}]
    $$
    - $\mathbf{W} \in \mathbb{R}^{T' \times d}$: 학습 가능한 가중치 행렬


<figure>
  <div style="text-align:center">
    <img src="/assets/img/t_mass/fig3.png" alt="Fig 3" style="width:70%;">
  </div>
</figure>


##### **Learning Text Mass in Joint Space**
- 기존 Loss Function의 한계
  - 기존 symmetric cross-entropy loss $\mathcal{L}_{ce}$ 는 text mass의 크기 조절이 어려움
  - 단일 텍스트 임베딩 $\mathbf{t}$ 만 학습하여 텍스트-비디오 정렬이 제한적
  - 확률적 텍스트 임베딩 $\mathbf{t}_s$ 를 샘플링하여 다양한 표현을 학습할 필요가 있음

- 확률적 텍스트 임베딩 학습
  - 확률적 샘플링을 적용하여 기존 손실 함수 개선:
  $$
  \mathbf{t}_s = \mathbf{t} + R \cdot \epsilon, \quad \epsilon \sim \mathcal{N}(0,1)
  $$
  - 새로운 손실 함수:
  $$
  \mathcal{L}_{\text{total}} = \mathcal{L}_{ce} + \mathcal{L}_s
  $$
    - $\mathcal{L}_s$: 확률적 텍스트 임베딩 $\mathbf{t}_s$ 를 활용한 손실
- 텍스트 표현 다양성 증가 → 일반화 성능 향상
  단일 $\mathbf{t}$ 가 아닌 다양한 표현을 학습 → 비디오와 정렬 문제 완화

- Support Text Regularization 추가
  - Support Text Embedding ($\mathbf{t}_{\text{sup}}$) 도입:
  $$
  \mathbf{t}_{\text{sup}} = \mathbf{t} + \frac{\mathbf{v} - \mathbf{t}}{|\mathbf{v} - \mathbf{t}|} R
  $$
- 추가 손실 함수 적용:
  $$
  \mathcal{L}_{\text{total}} = \mathcal{L}_s + \alpha \mathcal{L}_{\text{sup}}
  $$
  - 텍스트 질량의 크기 및 이동 조절 → 학습 안정성 향상


<figure>
  <div style="text-align:center">
    <img src="/assets/img/t_mass/fig4.png" alt="Fig 4" style="width:70%;">
  </div>
</figure>

##### **Inference Pipeline**
- T-MASS의 확률적 텍스트 표현을 활용하여 **추론 과정(inference pipeline)을 개선**.
- 기존 방법은 고정된 텍스트 임베딩 $\mathbf{t}$ 를 사용했지만, **T-MASS는 확률적 샘플링을 통해 최적의 표현을 동적으로 선택**.

1. **텍스트 및 비디오 특징 추출**
   - 주어진 텍스트-비디오 쌍 ${t, v}$ 에 대해 features인 $\mathbf{v}, [\mathbf{f}_1, ..., \mathbf{f}_{T'}]$을 추출
     $$
     \mathbf{f}_i = \phi_v(f_i), \quad i = 1, ..., T'
     $$
     $$
     \mathbf{t} = \phi_t(t)
     $$
   - 이후, 비디오 임베딩을 생성:
     $$
     \mathbf{v} = \psi([\mathbf{f}_1, ..., \mathbf{f}_{T'}], \mathbf{t})
     $$

2. **확률적 텍스트 임베딩 샘플링**
  - 기존 단일 텍스트 임베딩 대신, **확률적 샘플링을 적용하여 $M$ 개의 텍스트 임베딩 생성**:
  - $\mathbf{t}_s = \mathbf{t} + R \cdot \epsilon$ 에 따라 $M$번의 stochastic sampling을 진행함

     $$
     \{\mathbf{t}_1^s, ..., \mathbf{t}_M^s\}, \quad \mathbf{t}_i^s = \mathbf{t} + R \cdot \epsilon_i, \quad \epsilon_i \sim \mathcal{N}(0,1)
     $$
   - **다양한 표현을 탐색하여 최적의 텍스트-비디오 정렬을 수행**.

3. **최적의 텍스트 임베딩 선택**
  - Video와 가장 similarity가 높은 text embedding을 선택:
     $$
     \hat{\mathbf{t}}_s = \arg \max_{\mathbf{t}_s} s(\mathbf{t}^i_s, \mathbf{v}), \quad i = 1, ..., M
     $$
  - Feature fusion module $\psi(\cdot)$을 통해 video feature $\mathbf{v}$ 계산  
  - Final text embedding $\hat{\mathbf{t}}_s$는 확률적 샘플링된 임베딩 집합에서 선택됨  
  - Video에 따라 adaptive 변화하는 임베딩 생성  
    - 기존 고정된 텍스트 임베딩보다 더 유연한 표현 가능  
    - 비디오와 더 유사한 텍스트 임베딩 탐색  
  - 텍스트-비디오 및 비디오-텍스트 검색 모두 적용 가능  

4. **T-MASS를 활용한 향상된 정렬 및 의미 적응**
  - 텍스트-비디오 정렬(text-video alignment) 개선  
  - 텍스트 의미 적응(text semantics adaptation) 지원  
  
---

### 3. Experiment

#### 3.1. 성능 비교
- 기존 방식 대비 **R@1 성능 3%~6.3% 향상**
- **MSRVTT, LSMDC, DiDeMo, Charades, VATEX 등 5개 벤치마크에서 SOTA 달성**
<br>

#### 3.2. Ablation Study
- **유사도 기반 반경 조절 모듈 추가 시 성능 향상 확인**
- **Support text regularization 적용 시 retrieval 성능 개선**
<br>

#### 3.3. 의미 정렬 분석
- 기존 방식 대비, **T-MASS는 텍스트-비디오 embedding 간의 의미적 정렬이 더 잘 이루어짐을 실험적으로 확인**
- 특히 **복잡한 영상일수록 T-MASS가 더 높은 성능을 보임**

---

## Conclusion
- 기존 text-video retrieval의 한계를 극복하기 위해 **확률적 텍스트 모델링 방법인 T-MASS를 제안**
- **텍스트를 단일 포인트가 아닌 질량으로 모델링하여 보다 풍부한 의미 표현 가능**
- **유사도 기반 반경 조절 모듈과 support text regularization을 활용하여 학습 효과 극대화**
- 기존 대비 높은 성능을 기록하며, 다양한 벤치마크 데이터셋에서 SOTA 달성
- 향후 연구 방향으로는 **비디오 내 프레임 간 의미 변화를 보다 정교하게 반영


### Abstract

