---
layout: post
title: "[Paper review] Text Is MASS: Modeling as Stochastic Embedding for Text-Video Retrieval"
last_modified_at: 2025-01-16
mathjax: true
image:  /assets/images/blog/post-5.jpg
categories:
  - 논문리뷰
tags:
  - ViT
  - Computer Vision
  - Video
  - AI
excerpt: "Text is Mass paper review"
use_math: true
classes: wide
---
> arXiv 2023. [[Paper](https://arxiv.org/pdf/2403.17998)] [[Github](https://github.com/Jiamian-Wang/T-MASS-text-video-retrieval)]  
> Jiamian Wang, Guohao Sun, Pichao Wang, Dongfang Liu, Sohail Dianat, Majid Rabbani, Raghuveer Rao, Zhiqiang Tao  
> Rochester Institute of Technology | Amazon Prime Video | Army Research Laboratory  
> CVPR 2024


Text Is MASS: Modeling as Stochastic Embedding for Text-Video Retrieval

### 0. Abstract
- 기존 text-video retrieval 방식에서 단일 텍스트 embedding이 비디오의 복잡한 의미를 충분히 표현하기 어려운 문제를 해결하기 위해 새로운 확률적 텍스트 모델링 기법(T-MASS) 제안
- T-MASS는 텍스트를 단일 포인트가 아닌 확률적 text mass로 모델링하여 보다 유연하고 풍부한 의미 표현이 가능하도록 함
- Similarity-aware radius module 도입으로 text-video pair에 맞춰 text mass의 크기를 동적으로 조절
- Support text regularization을 통해 학습 시 text mass를 효과적으로 제어
- T-MASS는 기존 방식 대비 R@1에서 3~6% 성능향상 및 MSRVTT, LSMDC, DiDeMo, Charades, VATEX 데이터셋에서 SOTA 달성

---

<figure>
  <div style="text-align:center">
    <img src="/assets/img/t_mass/fig1.png" alt="Fig 1" style="width:70%;">
  </div>
</figure>

### 1. Introduction
영상 데이터의 폭발적인 증가로 인해 text-video retrieval 연구가 활발히 진행되고 있다. 기존 접근 방식은 텍스트와 비디오를 동일한 embedding 공간에 매핑하여 유사도를 계산하는 방식이 주류를 이루고 있다. 하지만 기존 데이터셋의 텍스트는 짧고 간결하여 비디오의 다양한 의미를 온전히 반영하기 어려운 문제가 있다.

이러한 한계를 극복하기 위해 본 연구에서는 텍스트 embedding을 단일 포인트가 아닌 확률적 질량(probabilistic mass)으로 모델링하는 **T-MASS (Text Modeled As a Stochastic embedding)** 방법을 제안한다. 이를 통해:
1. **비디오의 다양한 의미를 보다 유연하게 표현 가능**
2. **텍스트-비디오 embedding 간의 alignment 문제 완화**
3. **불확실성까지 반영하는 효과적인 검색 가능**

기존 방법과의 차별점으로, T-MASS는 단순한 텍스트 포인트가 아닌 분포(distribution) 형태로 텍스트를 모델링하며, 유사도 기반 반경 조절 모듈(similarity-aware radius module)과 support text vector를 활용하여 효과적인 학습 및 추론을 수행한다.

---

### 2. Method
#### 2.1. Preliminaries
- 텍스트를 $t$, 원본 비디오 클립을 $v$라고 정의
- Text-video retrieval은 $t, v \in \mathbb{R}^d$의 공통 embedding 공간을 학습하는 과정
- Cosine similarity 등 similarity 측정 함수 $s(t, v)$를 사용하여 텍스트-비디오 간 연관성을 평가
- 손실 함수는 **Symmetric Cross Entropy**를 사용하여 관련된 쌍은 가깝게, 무관한 쌍은 멀어지도록 학습

$$
\begin{equation}
L_{t \to v} = -\sum_{i=1}^{N} \log \frac{e^{s(t_i, v_i) \cdot \lambda}}{\sum_{j} e^{s(t_i, v_j) \cdot \lambda}} \\
L_{v \to t} = -\sum_{i=1}^{N} \log \frac{e^{s(t_i, v_i) \cdot \lambda}}{\sum_{j} e^{s(t_j, v_i) \cdot \lambda}} \\
L_{ce} = \frac{1}{2} (L_{t \to v} + L_{v \to t})
\end{equation}
$$

- 손실 함수는 모든 관련 텍스트-비디오 쌍의 유사도가 1, 무관한 쌍의 유사도가 0이 되는 이상적인 상태를 목표로 함

#### 2.2. Text-Video Representations

##### Feature Extraction
- CLIP 기반의 multu-modal embedding을 활용하여 텍스트와 비디오를 공통 공간으로 변환
- 비디오는 $T$ 개의 프레임으로 구성되며, $T'$ 개의 프레임을 샘플링하여 CLIP을 통해 임베딩 추출

$$
\begin{equation}
f_i = \varphi_v(f_i), \quad i=1, \dots, T' \\
t = \varphi_t(t)
\end{equation}
$$

- 기존 연구들은 비디오 프레임 임베딩을 다양한 방식으로 결합하여 최종 비디오 임베딩을 생성함

$$
\begin{equation}
v = \psi([f_1, ..., f_{T'}], t)
\end{equation}
$$

- 기존 방법들은 주로 **비디오 임베딩 학습**에 초점을 맞추었으나, 텍스트 표현력 부족 문제가 발생함

##### Motivation
- 기존 연구들은 **비디오 임베딩 학습($v$)에 집중**했으며, 텍스트 표현력 부족 문제를 충분히 해결하지 못함
- 텍스트 $t$는 비디오 $v$에 비해 표현력이 낮으며, 비디오가 제공하는 풍부한 단서를 온전히 반영하기 어려움
- 이를 해결하기 위해 **텍스트를 단일 포인트가 아닌 확률적 질량(text mass)으로 모델링하는 방법을 제안**

#### 2.3. Proposed Method: T-MASS
##### Stochastic Text Modeling
- T-MASS는 텍스트를 단일 포인트가 아닌 "확률적 질량"(text mass)으로 모델링하여 보다 표현력이 높은 임베딩을 학습함
- 기존의 정적인 텍스트 임베딩 방식과 달리, $t_s = t + R \cdot \epsilon, \epsilon \sim P$을 이용하여 확률적 표현을 생성
- $R$은 텍스트 질량의 크기를 정의하며, 학습을 통해 최적화됨

##### Similarity-Aware Radius Modeling
- 유사도 기반 반경 조절 모듈을 활용하여 텍스트 질량의 크기를 동적으로 조절
- 텍스트-비디오 쌍의 유사도를 계산하고 이를 반경 조절의 지표로 활용하여 효과적인 검색 가능

##### Learning Text Mass in Joint Space
- 기존의 손실 함수 $L_{ce}$를 개선하여 확률적 텍스트 임베딩 $t_s$를 활용하는 새로운 학습 방법 제안
- 학습 중 여러 샘플을 생성하여 다양한 텍스트 표현이 훈련에 참여하도록 유도
- Support text regularization을 추가하여 학습 안정성을 향상

##### Inference Pipeline
- 추론 과정에서 여러 개의 확률적 텍스트 임베딩을 생성한 후, 비디오와 가장 유사한 임베딩을 선택하여 최종 검색 수행


---

### 3. Experiment

#### 3.1. 성능 비교
- 기존 방식 대비 **R@1 성능 3%~6.3% 향상**
- **MSRVTT, LSMDC, DiDeMo, Charades, VATEX 등 5개 벤치마크에서 SOTA 달성**

#### 3.2. Ablation Study
- **유사도 기반 반경 조절 모듈 추가 시 성능 향상 확인**
- **Support text regularization 적용 시 retrieval 성능 개선**

#### 3.3. 의미 정렬 분석
- 기존 방식 대비, **T-MASS는 텍스트-비디오 embedding 간의 의미적 정렬이 더 잘 이루어짐을 실험적으로 확인**
- 특히 **복잡한 영상일수록 T-MASS가 더 높은 성능을 보임**

---

## Conclusion
- 기존 text-video retrieval의 한계를 극복하기 위해 **확률적 텍스트 모델링 방법인 T-MASS를 제안**
- **텍스트를 단일 포인트가 아닌 질량으로 모델링하여 보다 풍부한 의미 표현 가능**
- **유사도 기반 반경 조절 모듈과 support text regularization을 활용하여 학습 효과 극대화**
- 기존 대비 높은 성능을 기록하며, 다양한 벤치마크 데이터셋에서 SOTA 달성
- 향후 연구 방향으로는 **비디오 내 프레임 간 의미 변화를 보다 정교하게 반영


### Abstract

