---
layout: post
title: "[Paper review] Localizing Objects with Self-Supervised Transformers
and no Labels"
last_modified_at: 2025-01-15
mathjax: true
image:  /assets/images/blog/post-5.jpg
categories:
  - 논문리뷰
tags:
  - ViT
  - Computer Vision
  - Self-Supervised Learning
  - AI
excerpt: "LOST paper review"
use_math: true
classes: wide
---


> [[Paper](https://arxiv.org/abs/2109.14279)] [[Github](https://github.com/valeoai/LOST)]  
> Oriane Sim´ eoni, Gilles Puy, Huy V. Vo, Simon Roburin, Spyros Gidaris1, Andrei
Bursuc, Patrick P´erez, Renaud Marlet and Jean Ponce
> Valeo.ai |  Inria | LIGM | Center for Data Science, New York University  
> BMVC 2021

<figure>
  <div style="text-align:center">
    <img src="/assets/img/lost/fig1.png" alt="Fig 1" style="width:90%;">
  </div>
</figure>

### Introduction
Object detector는 자율주행 차량과 같은 핵심 시스템에서 중요한 역할을 한다. 하지만 높은 성능을 달성하기 위해서는 대량의 annotated data를 필요로 한다. 이러한 비용을 줄이기 위한 다양한 접근법이 제안되었으며, 예를 들어 semi-supervision, weak supervision, active learning, 그리고 self-supervision 기반의 task fine-tuning 등이 있다.

이 연구에서는 **annotation 없이 이미지를 통해 object의 localizing**하는 방법을 다룬다. 초기 연구들은 saliency 또는 이미지 내 유사성(intra-image similarity)만을 활용했으나, 이는 정확도가 낮고 제안된 영역이 너무 많아 대규모 데이터셋에는 적합하지 않았다. 일부 접근법은 annotation 없이 extra modalities(e.g., audio, LiDAR) 등을 활용하기도 했다.

이에 대해, 논문에서 **unsupervised 방식으로 단일 이미지 수준에서 object를 localizing**하는 간단한 방법을 제안한다. 이 방법은 이미지 간 유사성을 탐색하지 않기 때문에 선형적인 복잡도를 가지며, 대규모 데이터셋에서도 높은 확장성을 가진다.
<br>

#### Idea
**[DINO](https://arxiv.org/pdf/2104.14294)**에서 학습된 고품질 특징을 활용한다. 구체적으로:
1. 이미지를 동일한 크기의 패치로 나눈다.
2. DINO 모델을 통해 각 패치의 특징을 추출한다.
3. 마지막 attention 레이어에서 key component를 사용하여여 패치 간 유사도를 계산한다.
4. **유사한 패치 수가 가장 적은 패치(Seed)**를 선택해 object의 일부를 localize한다. 이는 foreground objects의 패치가 background 패치보다 상관성이 낮다는 empirical criterion 기반했다.
5. 초기 Seed에 대해 유사성이 높은 패치를 추가하며, 이를 **Seed 확장(Seed Expansion)**이라고 힌다.
6. 최종적으로 이 과정을 통해 **binary object segmentation mask**를 생성하고, 연결된 가장 큰 구성 요소에 대해 **bounding box**를 추론한다.

<br>

#### Contribution
이 연구는 다음과 같은 주요 성과를 달성했다.
1. Self-supervised pre-trained Vision Transformer에서 추출한 feature와 patch 간의 상관관계를 활용하여, region proposals 및 기존의 single-object discovery 방법을 능가하는 성능을 보이는 간단한 single-object localization 방법을 제안했다.
2. 제안된 방법론을 활용하여 클래스에 구애받지 않는 (class-agnostic) 객체 탐지기를 학습시켰으며, 이를 통해 단일 이미지에서 다중 객체를 정확히 탐지할 수 있었다.
3. 탐지된 객체를 시각적으로 일관된 클래스로 클러스터링하여 class-aware object detector를 학습시켰다.
4. 일부 클러스터는 데이터셋의 라벨된 의미적 클래스와 높은 상관성을 보여, weakly-supervised 학습 수준의 객체 탐지 결과를 달성했다.

이 연구는 비지도 학습의 잠재력을 보여주며, 대규모 데이터셋에 주석을 달지 않고도 정확한 객체 탐지를 가능하게 한다는 점에서 큰 의미를 갖는다.

---

### Proposed approach

#### Transformers for Vision
**Input**  
Vision Transformers는 고정 크기의 패치 $P \times P$ 시퀀스를 입력으로 사용한다. 색상 이미지 $I$의 공간 크기가 $H \times W$일 때, 패치의 개수는 다음과 같이 계산된다.

$$
\begin{equation}
N = \frac{H \cdot W}{P^2}
\end{equation}
$$

여기서 각 패치는 $3P^2$ 크기를 가지며, $H$와 $W$는 $P$의 배수라고 가정한다. 각 patch는 학습된 linear projection layer을 통해 $d$-dimension의 embedding space로 mapping된다. 또한, 학습된 벡터인 “class token” $\text{CLS}$가 patch embedding에 추가되어 Transformer의 입력은 $\mathbb{R}^{(N+1) \times d}$에 속하게 된다.
<br>

**Self-Attention**  
Transformer는 다중 헤드 self-attention layer와 MLP들로 구성된다. Self-attention layer의 입력 $X \in \mathbb{R}^{(N+1) \times d}$에 대해 다른 linear transformation이 적용되어 query $(Q)$, key$(K)$, value$(V)$를 생성한다.$Q, K, V \in \mathbb{R}^{(N+1) \times d}$

Self-attention 레이어의 출력은 다음과 같이 계산된다.

$$
Y = \text{softmax} \left( d^{-1/2} Q K^\top \right) V \in \mathbb{R}^{(N+1) \times d}
$$

여기서 softmax는 행(row) 단위로 적용됩니다. 단순화를 위해 단일 헤드 attention 레이어의 경우를 설명했지만, 실제 attention 레이어는 일반적으로 multi-head를 포함한다. 이 연구에서는 Transformer의 마지막 self-attention layer에서 각 헤드의 key, query, value를 concatenate하여 feature representation을 생성한다.
<br>

**Features for Object Localization**  
이 연구에서는 DINO로 self-supervised 방식으로 pretrained Transformers를 사용한다. DINO는 마지막 attention 레이어에서 CLS query의 self-attention을 통해 유의미한 객체 분할을 얻을 수 있다. 이를 기반으로 'DINO-seg'라는 baseline을 구축하여 object detection을 수행한다.

그러나 저자들은 이 방법으로는 self-supervised Transformer features의 potential을 충분히 활용하지 못한다고 판단했다. 이에 따라, 이 연구에서는 패치 간 similarity를 계산하고 이를 활용하여 object를 더 효과적으로 localize하는 새로운 방법인 LOST를 제안한다. LOST는 Transformer의 마지막 레이어에서 추출한 patch key $k_p \in \mathbb{R}^d, \, p = 1, \ldots, N$를 사용하여 단일 이미지의 패치 간 유사도를 계산함으로써 작동한다.


<figure>
  <div style="text-align:center">
    <img src="/assets/img/lost/fig2.png" alt="Fig 1" style="width:80%;">
  </div>
</figure>
<br>

